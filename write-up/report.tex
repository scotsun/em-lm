% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  twocolumn]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Enhancing Linear Regression with EM algorithm to Handle Missing Data},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{Enhancing Linear Regression with EM algorithm to Handle
Missing Data}}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{BIOSTAT 823 Final Project Report}
\author{Scott Sun\\
GitHub repo: \url{https://github.com/scotsun/bios823final_project}}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Missing data is a common issue in data analysis. Unbiased and efficient
estimation of the parameters governing the mean response model requires
the missing data to be appropriately addressed. In this project, we
derived and developed an EM algorithm for General Linear Model and
compared its performance with Complete-Case Analysis. The simulation
result showed that applying the EM algorithm over the full likelihood
was more preferable than applying it over the observed variables'
likelihood. More importantly, EM estimators were generally more
efficient and less biased when the missingness pattern became more
complex.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

It is common for applied data analysts or researchers to retain complete
observations only when the data has missingness. This approach is called
complete-case (CC) analysis. It is usually the default in many software
packages (i.e., \texttt{lm} in \texttt{R}). It is straightforward to
understand but only sometimes valid since it may exclude potentially
helpful information. Various statistical or machine learning approaches
handle missingness have been developed as more justified alternatives to
CC. In this project, we will derive and develop a method called
\textbf{EM-LM}, which combines the Expectation-Maximization (EM)
algorithm and General Linear Model. Meanwhile, we will also compare the
performance of EM-LM with \texttt{R}'s default \texttt{lm} under varied
conditions of degrees of missingness, noise scales, predictor being
misspecified or not, and missingness mechanisms. We will demonstrate the
comparisons through numerical simulations.

\hypertarget{problem-setting-and-notations}{%
\section{Problem Setting and
Notations}\label{problem-setting-and-notations}}

Assume in the General Linear Model, the sample size is \(n\) and the
number of predictors is \(p\). In this project, we will only discuss
scenarios where the predictor variable \(X \in \mathbb{R}^{n \times p}\)
can have missing values and the response variable \(y \in \mathbb{R}^n\)
is fully observed. On the observation level, \(x_i\) is p-vector and
\(y_i\) is a scalar. We use \(R \in \mathbb{R}^{n \times p}\) and
\(r \in \mathbb{R}^{n \times 1}\) to indicate the observation of every
data value and the completeness of observation, respectively. Let \(S\)
represent the index set of all individuals of complete observation, and
let \(\bar{S}\) represent the complementary set. That is,
\(S = \{i: r_i = 1\}\) and \(\bar{S} = \{i: r_i = 0\}\).

\hypertarget{missingness-mechanisms}{%
\section{Missingness Mechanisms}\label{missingness-mechanisms}}

In the framework defined by Little and Rubin
{[}\protect\hyperlink{ref-little2019statistical}{1}{]}, missingness
mechanisms are classified into three categories: \textbf{Missing
Completely at Random (MCAR)}, \textbf{Missing at Random (MAR)}, and Not
Missing at Random (NMAR). Any missingness pattern following the MCAR or
MAR mechanism is called \textbf{ignorable} because the pattern does not
interfere with parameter estimation and inference
{[}\protect\hyperlink{ref-little2019statistical}{1}{]}. When the
missingness follows NMAR, it becomes non-ignorable. However, within this
project's scope, we will only discuss MCAR and MAR, as EM-LM will only
be justified under these two settings.

\hypertarget{missing-completely-at-random}{%
\subsection{Missing Completely at
Random}\label{missing-completely-at-random}}

If the likelihood of being observed is independent of any variables, the
missingness mechanism is MCAR. That is, \[
\Pr(R|X,  \boldsymbol\Theta) = \Pr(R| \boldsymbol\Theta).
\] MCAR is a strong ideal assumption on the mechanism, which assumes the
missingness is unrelated to all variables. It indicates that the pattern
is not affected by the studied subjects. For instance, when we research
blood samples, several samples can be contaminated during the delivery
process. Thus, we cannot collect information from these polluted
specimens. Nothing about the specimens themselves made them more or less
likely to be contaminated.

\hypertarget{missing-at-random}{%
\subsection{Missing at Random}\label{missing-at-random}}

If the likelihood of being observed depends only on those fully observed
variables, the missingness mechanism is MAR. That is, \[
\Pr(R|X, \boldsymbol\Theta) = \Pr(R|X_{obs}, \boldsymbol\Theta).
\] Compared to MCAR, MAR is a less constrained statement. Based on this
definition, MCAR can be seen as an extreme, special case of MAR
{[}\protect\hyperlink{ref-schafer2002missing}{2}{]}.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{complete-case-analysis}{%
\subsection{Complete-Case Analysis}\label{complete-case-analysis}}

CC analysis is a common method in the presence of data missingness.
After removing the incomplete observations from the dataset, we can
perform all kinds of calculation on the remaining data. These
observations are indexed by elements in \(S\). For all \(i \in S\),
\(r_i = 1\). As a result, the likelihood function
\(L_{CC}(\boldsymbol{\Theta})\).\\
\[
\begin{aligned}
L_{CC}(\boldsymbol\Theta)
= \prod_{i \in S} \big[ f(y_i, x_i; \boldsymbol\Theta)f(z_i|y_i, x_i; \boldsymbol\Theta) \big]
\end{aligned}
\] Then, the CC analysis obtains \(\hat{\boldsymbol\Theta}\) by
maximizing \(L_{CC}(\boldsymbol\Theta)\). There are a few circumstances
where the CC analysis can yield valid estimators. When the missingness
mechanism is MCAR, the estimator is generally unbiased and consistent
because the completeness is independent of all variables in the data.
The complete cases can be viewed as random samples from an imagined full
data set {[}\protect\hyperlink{ref-seaman2013review}{3}{]}. Apart from
that, the CC analysis estimator can have negligible bias under a
relative weaker condition: the missingness mechanism is MAR, but it is
independent of the response variable given the observed predictor
variables {[}\protect\hyperlink{ref-seaman2013review}{3}{]}. CC analysis
can be straightforwardly implemented in coding by dropping observations
with missing data.

\hypertarget{maximum-likelihood-em-algorithm}{%
\subsection{Maximum Likelihood EM
Algorithm}\label{maximum-likelihood-em-algorithm}}

The EM algorithm is an iterative optimization process to calculate the
maximum likelihood estimates of parameters when there exist hidden
variables or missing data
{[}\protect\hyperlink{ref-dempster1977maximum}{4}{]}. As the name
directly tells, it consists of two major steps: expectation (E-step) and
maximization (M-step). In the E-step, we derive the conditional
expectation of the log-likelihood function. The expectation is
integrated over the conditional distribution:
\[f(\text{variables}~|~\text{observed variables})\] Then, in the M-step,
we iteratively optimize the conditional expectation to obtain the
maximum likelihood estimates. The M-step can be visualized as the
following pseudo-code.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Initial guess on the parameters}
\Output{MLE}
\BlankLine
initialization $\boldsymbol{\hat{\Theta}}^{(0)}$\;
\While{k < max-iteration}{
    $\boldsymbol{\hat{\Theta}}^{(k)} = argmax~\mathbb{E}_{\boldsymbol\Theta}[\log L|\boldsymbol{\hat{\Theta}}^{(k-1)}, X_{obs}]$\;
    $e = \boldsymbol{\hat{\Theta}}^{(k)} - \boldsymbol{\hat{\Theta}}^{(k-1)}$\;
    \eIf{$\max{|e|}$ < tolerance}{
        return $\boldsymbol{\hat{\Theta}}^{(k)}$\;
    }{
        continue\;
    }
}
\caption{M-step of EM algorithm}
\end{algorithm}

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

In our linear regression setting, any ignorable missingness can happen
to \(X\), but \(y\) is fully observed. Moreover, since the EM algorithm
maximize the joint distribution of \(x_i\) and \(y_i\), we have to
explicitly assume parametric model for both variables. Therefore, the
EM-LM algorithm assume the following distributions. \[
\begin{aligned}
x_i &\overset{iid}{\sim} \mathcal{N}(\mu, \Sigma) \\
y_i | x_i &\overset{ind}{\sim} N(\beta_0 + x^{\top}\beta, \sigma^2)
\end{aligned}
\] The likelihood function to be optimized falls into one of the
follows. \[
\begin{aligned}
&L_{full} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_i)\\
&L_{observed} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_{i,observed})
\end{aligned}
\]

\hypertarget{point-estimates}{%
\subsubsection{Point Estimates}\label{point-estimates}}

As the problem setting states, any element in \(X\) can be missing and
\(y\) is fully observed. To derive the conditional expectation of the
log-likelihood function, denoted as
\(J\big(\boldsymbol{\Theta}|\boldsymbol{\hat{\Theta}}^{(k-1)}\big)\), we
need to obtain the reverse conditional distribution \(f(x_i|y_i)\). \[
\begin{aligned}
f(x_i|y_i) &\propto f(y_i|x_i) f(x_i) \\
&\propto \exp\Big\{ -\frac{1}{2}x_{i}^{\top}\big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)x_i+
(\beta^{\top} \frac{y-\beta_0}{\sigma^2} + \mu^{\top}\Sigma^{-1})x_i\Big\}
\end{aligned}
\] Thus, \[
\begin{aligned}
x_i | y_i &\overset{Ind}{\sim} \mathcal{N}(\xi_i, \Phi), \\
&\text{where}
\begin{cases}
    \Phi = \big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)^{-1} \\
    \xi_i = \big( \frac{y_i-\beta_0}{\sigma^2}\beta + \Sigma^{-1}\mu \big)\Phi
\end{cases}
\end{aligned}
\] One interesting observation from the derivation is that independence
among predictor variables does not guarantee conditional independence
given the response.

Now, we can define the conditional expectation function as
\(\tilde{\mathbb{E}}(\cdot)\). \[
\tilde{\mathbb{E}}(g(x)) = 
\begin{cases}
    g(x), ~\text{if}~x~\text{observed} \\
    \mathbb{E}[g(x) |~\text{observed data}, \boldsymbol{\hat\Theta}], ~\text{else}
\end{cases}
\] Optimizing \(J\) is equivalent to optimizing
\(J* = \tilde{\mathbb{E}}\big[\sum_{i=1}^{n} \log f(y_i|x_i) + \log f(x_i)\big]\).
Meanwhile, let \(\beta^{*} = (\beta_0, \beta)\) and
\(x_i^{*} = (1, x_i)\). Then, the goal became optimizing \[
\begin{aligned}
J* &= \sum_{i = 1}^{n}\tilde{\mathbb{E}}\Big\{ -\frac{1}{2} \log(|\Sigma|) - \frac{1}{2} (x_i - \mu)^{\top}\Sigma^{-1}(x_i - \mu) \\
&- \frac{1}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}(y - x^{*\top}\beta^{*})\Big\}
\end{aligned}
\] We can calculate the first derivatives to optimize the function. \[
\begin{aligned}
\boldsymbol{\mathrm{D}}_{{\beta}^{*}}J* &= 
\sum_{i=1}^{n} - \frac{1}{\sigma^2} \{-\tilde{\mathbb{E}}(x_i^{*})y_i + \tilde{\mathbb{E}}(x_i^{*}x_i^{*\top})\beta^{*}\} \\
\boldsymbol{\mathrm{D}}_{{\sigma}^2}J* &=
\sum_{i=1}^{n} \Big\{ - \frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} \tilde{\mathbb{E}}\big((y_i-x^{*\top}\beta^{*})^2\big) \Big\}\\
\boldsymbol{\mathrm{D}}_{\mu}J* &= \Sigma^{-1} \sum_{i=1}^{n} \Big\{\tilde{\mathbb{E}}(x_i) - \mu \Big\}\\
\boldsymbol{\mathrm{D}}_{\Sigma}J* &= 
\sum_{i=1}^{n} -\frac{1}{2} \Big\{ \Sigma^{-1} - \Sigma^{-1} \tilde{\mathbb{E}}\big((x_i-\mu)^2\big) \Sigma^{-1} \Big\}
\end{aligned}
\] By setting the first derivative to 0, we obtained the closed-form
estimator. For a simple linear regression case, we can replace the
\(\mu\) and \(\Sigma\) with scalars \(\mu_1\) and \(\sigma_{1}^{2}\). It
is helpful to use \emph{The Matrix Cook}
{[}\protect\hyperlink{ref-petersen2008matrix}{5}{]} as the reference to
support linear algebra calculation during the derivation.

\hypertarget{standard-error-confidence-interval}{%
\subsubsection{Standard Error \& Confidence
Interval}\label{standard-error-confidence-interval}}

The theoretical standard error of estimates are calculated from the
observed information matrix \[ 
I_n (\hat{\boldsymbol{\Theta}}) = n \cdot \mathrm{Cov}(S(\hat{\boldsymbol{\Theta}}))
\] Then the standard error of \(\hat\Theta_i\) is calculated by the
square root of the i-th diagonal element of
\(I_n(\hat{\boldsymbol{\Theta}})^{-1}\). Due to the asymptotic normality
of the maximum likelihood estimator, we can use the standard errors to
calculate \((1-\alpha)100\%\) confidence interval (CI) for parameters.
We also implement a bootstrapping approach based on Simple Random
Sampling with Replacement as an alternative way to calculate CIs.
Considering the computation efficiency, we will not use bootstrapped CI
in the simulation studies.

\hypertarget{simulation-studies}{%
\section{Simulation Studies}\label{simulation-studies}}

We designed a series of numerical simulations to evaluate the
performance of EM-LM and compare it with the CC approach. Each instance
of the simulated dataset is obtained from a two step procedure. In the
first step, we generate the full dataset from a response model as
follows. \[
y_i = \beta_0 + x_{i}^{\top}\beta + \epsilon_i
\] where \(i = 1,...,N\).
\(\forall i, x_i \sim \mathcal{N}(\mu, \Sigma)\) and
\(\epsilon_i \sim N(0, \sigma^2)\). In the second step, we generate the
\(n \times p\) random matrix \(R\) to indicate the observation of \(X\).
Each element in the matrix, \(R_{ij}\), follows a Bernoulli distribution
with probability \(p_{ij}\). If the missingness mechanism is MCAR,
\(p_{ij}\) is a constant for all \(i\); if the missingness mechanism is
MAR, \(p_{ij}\)'s values are calculated through a logistic regression
depending on other variables, such as \(y\) or any other column of
\(X\).

\hypertarget{simple-linear-regression}{%
\subsection{Simple Linear Regression}\label{simple-linear-regression}}

In the simulations of this section, the simulated data are based on a
fixed set of population parameters shown below. \[
\mu_1 = 10,~\sigma_{1}^{2} = 25,~\beta_0 = 10,~\beta_1 = 2
\] We define \(1 - p\) as the missingness rate and \(\sigma\) as the
noise scale. These two parameters are tuned in each instance of the
simulation.

\hypertarget{varied-missingness-rate-and-noise-scale}{%
\subsubsection{Varied Missingness Rate and Noise
Scale}\label{varied-missingness-rate-and-noise-scale}}

Tables 1-4 display the estimators' performance under varied missingness
rates noise scales.

\begin{table}[h]
\caption{Missingness rate is 5\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.001 & 24.989 & 24.717 & 10.016 & 1.998\\
\hspace{1em}full & 10.001 & 24.867 & 24.722 & 10.012 & 1.999\\
\hspace{1em}observed & 10.001 & 24.858 & 24.728 & 10.01 & 1.999\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.133 & 6.564 & 6.204 & 0.669 & 0.005\\
\hspace{1em}full & 0.128 & 6.302 & 6.193 & 0.662 & 0.005\\
\hspace{1em}observed & 0.133 & 6.495 & 6.198 & 0.662 & 0.005\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.946 & 0.946 & 0.942 & 0.95 & 0.948\\
\hspace{1em}full & 0.948 & 0.945 & 0.946 & 0.954 & 0.953\\
\hspace{1em}observed & 0.942 & 0.942 & 0.947 & 0.953 & 0.953\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 20\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 9.988 & 25.029 & 24.777 & 10.006 & 2\\
\hspace{1em}full & 9.987 & 24.909 & 24.796 & 9.99 & 2.001\\
\hspace{1em}observed & 9.988 & 24.872 & 24.831 & 9.981 & 2.002\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.159 & 7.985 & 7.945 & 0.785 & 0.006\\
\hspace{1em}full & 0.134 & 6.932 & 7.868 & 0.74 & 0.006\\
\hspace{1em}observed & 0.159 & 7.885 & 7.902 & 0.743 & 0.006\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.944 & 0.943 & 0.931 & 0.951 & 0.948\\
\hspace{1em}full & 0.951 & 0.945 & 0.94 & 0.952 & 0.952\\
\hspace{1em}observed & 0.923 & 0.928 & 0.942 & 0.952 & 0.951\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.013 & 25.029 & 24.474 & 9.995 & 2\\
\hspace{1em}full & 10.008 & 24.888 & 24.547 & 9.936 & 2.005\\
\hspace{1em}observed & 10.013 & 24.778 & 24.777 & 9.885 & 2.01\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.247 & 12.943 & 11.924 & 1.368 & 0.011\\
\hspace{1em}full & 0.149 & 8.538 & 11.804 & 1.167 & 0.009\\
\hspace{1em}observed & 0.247 & 12.684 & 12.227 & 1.234 & 0.01\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.949 & 0.94 & 0.926 & 0.942 & 0.943\\
\hspace{1em}full & 0.956 & 0.948 & 0.939 & 0.946 & 0.953\\
\hspace{1em}observed & 0.877 & 0.893 & 0.943 & 0.94 & 0.947\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 10}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 9.987 & 25.01 & 97.84 & 10.038 & 1.994\\
\hspace{1em}full & 10.001 & 24.844 & 98.356 & 9.959 & 2.005\\
\hspace{1em}observed & 9.987 & 24.759 & 99.034 & 9.987 & 2.003\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.253 & 12.293 & 206.502 & 5.039 & 0.041\\
\hspace{1em}full & 0.192 & 10.868 & 180.224 & 3.821 & 0.031\\
\hspace{1em}observed & 0.253 & 12.047 & 181.751 & 3.897 & 0.031\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.944 & 0.941 & 0.919 & 0.952 & 0.948\\
\hspace{1em}full & 0.948 & 0.947 & 0.933 & 0.954 & 0.954\\
\hspace{1em}observed & 0.912 & 0.93 & 0.939 & 0.951 & 0.951\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\hypertarget{misspecified-predictor-distribution}{%
\subsubsection{Misspecified predictor
distribution}\label{misspecified-predictor-distribution}}

A Normal distribution is used to model a right skewed data that was
originally designed to follow a \(Exp(\lambda = 0.1)\). We assess
estimators performance under misspecification with two missingness
rates: 5\% and 50\%. Tables 5-6 display estimators' performance under
varied missingness rates when \(X\) is misspecified.

\begin{table}[h]
\caption{Missingness rate is 5\%, noise scale is 5, and misspecified $X$}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 24.744 & 9.998 & 2\\
\hspace{1em}full & 24.746 & 9.997 & 2\\
\hspace{1em}observed & 24.753 & 9.995 & 2.001\\
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 6.458 & 0.272 & 0.001\\
\hspace{1em}full & 6.462 & 0.272 & 0.001\\
\hspace{1em}observed & 6.474 & 0.273 & 0.001\\
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.935 & 0.947 & 0.949\\
\hspace{1em}full & 0.939 & 0.95 & 0.955\\
\hspace{1em}observed & 0.939 & 0.951 & 0.953\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, noise scale is 5, and misspecified $X$}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 24.463 & 10.003 & 2\\
\hspace{1em}full & 24.53 & 9.963 & 2.005\\
\hspace{1em}observed & 24.968 & 9.884 & 2.017\\
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 12.094 & 0.513 & 0.003\\
\hspace{1em}full & 12.313 & 0.497 & 0.003\\
\hspace{1em}observed & 15.081 & 0.576 & 0.005\\
\addlinespace[0.3em]
\multicolumn{4}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.927 & 0.949 & 0.942\\
\hspace{1em}full & 0.938 & 0.957 & 0.932\\
\hspace{1em}observed & 0.941 & 0.944 & 0.863\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\hypertarget{multivariate-linear-regression}{%
\subsection{Multivariate Linear
Regression}\label{multivariate-linear-regression}}

In the previous simulation studies for simple linear regression, we did
experiments with MCAR missingness under various missingness rates and
noise scales as well as whether or not misspecifying predictor. Now, we
designed two different MAR mechanisms for \(X\) to test the performance
of the EM-LM in the multivariate linear regression setting. In
simulations of this section, the simulated data are based on a fixed set
of population parameters shown below. Moreover, we set the missingness
scale as 5 for both simulations. Thus, the only condition we will tune
is the missingness mechanism. \[
\begin{aligned}
\mu &= (10, 0, 10)^{\top} \\
\Sigma &= \text{diag}(25, 1, 25) \\
\beta_0 &= 10 \\
\beta &= (1, 3, 0)^{\top}
\end{aligned}
\]

\hypertarget{mar-independent-of-y}{%
\subsubsection{MAR independent of y}\label{mar-independent-of-y}}

In this simulation, the missingness mechanism is designed as follows.
Table 7 display estimators' performance under MAR missingness that is
independent of the response variable. \[
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -40 + X_{i2} \\
p_{i2} &= 0.8
\end{aligned}
\]

\begin{table*}[hbp]
\caption{Here you can type in your caption}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\mu_2$ & $\mu_3$ & $\sigma^2_1$ & $\sigma^2_2$ & $\sigma^2_3$ & $\sigma^2$ & $\beta_0$ & $\beta_1$ & $\beta_2$ & $\beta_3$\\
\midrule
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10 & 0 & 10.002 & 24.981 & 0.907 & 25.027 & 24.309 & 9.977 & 1 & 2.998 & 0.003\\
\hspace{1em}full & 9.825 & 0 & 10.002 & 25.024 & 0.996 & 24.867 & 25.619 & 10.41 & 0.975 & 2.694 & 0.002\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.151 & 0.005 & 0.16 & 9.089 & 0.012 & 9.286 & 8.976 & 1.769 & 0.008 & 0.211 & 0.008\\
\hspace{1em}full & 0.142 & 0.005 & 0.16 & 7.181 & 0.01 & 7.819 & 6.982 & 1.222 & 0.006 & 0.13 & 0.005\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.948 & 0.949 & 0.947 & 0.923 & 0.787 & 0.926 & 0.917 & 0.946 & 0.946 & 0.949 & 0.945\\
\hspace{1em}full & 0.929 & 0.954 & 0.95 & 0.951 & 0.951 & 0.949 & 0.972 & 0.964 & 0.952 & 0.904 & 0.982\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table*}

\hypertarget{mar-dependent-of-y}{%
\subsubsection{MAR dependent of y}\label{mar-dependent-of-y}}

In this simulation, the missingness mechanism is designed as follows.
Table 8 display estimators' performance under MAR missingness that
depends on the response variable. \[
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -12 + y \\
p_{i2} &= 0.8
\end{aligned}
\]

\begin{table*}[hbp]
\caption{Here you can type in your caption}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\mu_2$ & $\mu_3$ & $\sigma^2_1$ & $\sigma^2_2$ & $\sigma^2_3$ & $\sigma^2$ & $\beta_0$ & $\beta_1$ & $\beta_2$ & $\beta_3$\\
\midrule
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.898 & -0.002 & 9.996 & 21.331 & 0.948 & 24.895 & 19.77 & 12.985 & 0.817 & 2.446 & -0.002\\
\hspace{1em}full & 10.005 & -0.002 & 9.996 & 24.923 & 0.995 & 24.794 & 25.659 & 10.25 & 0.976 & 2.81 & 0\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.134 & 0.005 & 0.168 & 6.973 & 0.013 & 8.853 & 5.842 & 1.51 & 0.007 & 0.167 & 0.006\\
\hspace{1em}full & 0.155 & 0.005 & 0.168 & 9.223 & 0.009 & 7.552 & 8.953 & 1.176 & 0.006 & 0.136 & 0.005\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.289 & 0.949 & 0.938 & 0.599 & 0.857 & 0.927 & 0.419 & 0.349 & 0.433 & 0.711 & 0.955\\
\hspace{1em}full & 0.946 & 0.954 & 0.942 & 0.943 & 0.951 & 0.947 & 0.964 & 0.982 & 0.953 & 0.949 & 0.982\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table*}

\hypertarget{discussion-conclusion}{%
\section{Discussion \& Conclusion}\label{discussion-conclusion}}

The simulation results corroborated our expectations that linear
regression augmented by the EM algorithm generally outperformed CC
analysis. It maximized the usage of all observed information without
dropping any incomplete observations. The simulation results provided us
with a few critical take-home messages. We realized that high
missingness rates require more iterations of M-steps to have the
algorithm achieve a convergence. Moreover, missingness, along with the
noise, weakened the estimator's efficiency. Based on the simple linear
regression setting, we found that applying EM over full likelihood
generally performs better than using it over the observed likelihood.
Implementing the full likelihood, we had higher coverage probability on
the predictor parameters and slightly better efficiency in estimating
linear model coefficients. ML approach with the EM algorithm was always
more efficient than CC, and the difference became more apparent when the
degree of missingness increased.

An interesting observation was that the misspecification of the
predictor's distribution did not significantly impair the estimation on
\(\beta\) for both methods. It was reasonable that CC was not affected
as the ordinary linear regression did not require any assumption on
\(X\), and CC was theoretically unbiased when the missingness mechanism
is MCAR. On the contrary, the EM algorithm demanded explicit modeling of
the likelihood function. It involved the distribution of \(X\). In the
derivation, the estimation of \(\beta\) depended on \(\mu\) and
\(\Sigma\). Theoretically, incorrect estimation of these predictor
parameters can eventually influence the estimation of \(\beta\).
However, the EM estimator seemed to be more robust than it was expected.
The bias was negligible in practice.

In the multivariate setting, as we could manipulate more predictor
variables, we assessed the algorithm performance under two different MAR
mechanisms: MAR independent of \(y\) and MAR dependent of \(y\).
Following Seaman and White's theory
{[}\protect\hyperlink{ref-seaman2013review}{3}{]}, we also observed that
CC estimator's bias is negligible when the mechanism did not involve
\(y\). However, its results became misleading when the mechanism
involved \(y\). EM-LM's results were generally more efficient. However,
it remained unclear that the estimates tended to be slightly biased, but
the converge probabilities were generally at the right level.

\hypertarget{reference}{%
\subsection*{Reference}\label{reference}}
\addcontentsline{toc}{subsection}{Reference}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-little2019statistical}{}}%
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{R.J. Little, D.B. Rubin, Statistical analysis with
missing data, John Wiley \& Sons, 2019.}

\leavevmode\vadjust pre{\hypertarget{ref-schafer2002missing}{}}%
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{J.L. Schafer, J.W. Graham, Missing data: Our view of the
state of the art., Psychological Methods. 7 (2002) 147.}

\leavevmode\vadjust pre{\hypertarget{ref-seaman2013review}{}}%
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{S.R. Seaman, I.R. White, Review of inverse probability
weighting for dealing with missing data, Statistical Methods in Medical
Research. 22 (2013) 278--295.}

\leavevmode\vadjust pre{\hypertarget{ref-dempster1977maximum}{}}%
\CSLLeftMargin{{[}4{]} }%
\CSLRightInline{A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum
likelihood from incomplete data via the EM algorithm, Journal of the
Royal Statistical Society: Series B (Methodological). 39 (1977) 1--22.}

\leavevmode\vadjust pre{\hypertarget{ref-petersen2008matrix}{}}%
\CSLLeftMargin{{[}5{]} }%
\CSLRightInline{K.B. Petersen, M.S. Pedersen, others, The matrix
cookbook, Technical University of Denmark. 7 (2008) 510.}

\end{CSLReferences}

\end{document}
