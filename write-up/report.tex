% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  twocolumn]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Enhancing Linear Regression with EM algorithm to Handle Missing Data},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{\textbf{Enhancing Linear Regression with EM algorithm to Handle
Missing Data}}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{BIOSTAT 823 Final Project Report}
\author{Scott Sun\\
GitHub repo: \url{https://github.com/scotsun/bios823final_project}}
\date{}

\begin{document}
\maketitle
\begin{abstract}
Missing data is a common issue in data analysis. Unbiased and efficient
estimation of the parameters governing the mean response model requires
the missing data to be appropriately addressed. In this project, we
derived and developed an EM algorithm for General Linear Model and
compared its performance with Complete-Case Analysis. The simulation
result shows that applying the EM algorithm over the full likelihood
rather than the observed variables' likelihood is preferable. More
importantly, EM estimators are generally more efficient and less biased
when the missingness pattern becomes more complex.
\end{abstract}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

It is common for applied data analysts or researchers to retain complete
observations only when the data has missingness. This approach is called
complete-case (CC) analysis. It is usually the default in many software
packages (i.e., \texttt{lm} in \texttt{R}). It is straightforward to
understand but only sometimes valid since it may exclude potentially
helpful information. Various statistical or machine learning approaches
handle missingness as more justified alternatives to CC. In this
project, we will derive and develop a method called \textbf{EM-LM},
which combines the Expectation-Maximization (EM) algorithm and General
Linear Model. Meanwhile, we will also compare the performance of EM-LM
with \texttt{R}'s default \texttt{lm} under varied conditions of degrees
of missingness, noise scales, and missingness mechanisms. We will
demonstrate the comparisons through numerical simulations.

\hypertarget{problem-setting-and-notations}{%
\section{Problem Setting and
Notations}\label{problem-setting-and-notations}}

Assume in the General Linear Model, the sample size is \(n\) and the
number of predictors is \(p\). In this project, we will only discuss
scenarios where the predictor variable \(X \in \mathbb{R}^{n \times p}\)
can have missing values and the response variable \(y \in \mathbb{R}^n\)
is fully observed. On the observation level, \(x_i\) is p-vector and
\(y_i\) is a scalar. We use \(R \in \mathbb{R}^{n \times p}\) and
\(r \in \mathbb{R}^{n \times 1}\) to indicate the observation of every
data value and the completeness of observation, respectively. Let \(S\)
represent the index set of all individuals of complete observation, and
let \(\bar{S}\) represent the complementary set. That is,
\(S = \{i: r_i = 1\}\) and \(\bar{S} = \{i: r_i = 0\}\).

\hypertarget{missingness-mechanisms}{%
\section{Missingness Mechanisms}\label{missingness-mechanisms}}

In the framework defined by Little and Rubin
{[}\protect\hyperlink{ref-little2019statistical}{1}{]}, missingness
mechanisms are classified into three categories: \textbf{Missing
Completely at Random (MCAR)}, \textbf{Missing at Random (MAR)}, and Not
Missing at Random (NMAR). Any missingness pattern following the MCAR or
MAR mechanisms is defined as \textbf{ignorable} because the pattern does
not interfere with parameter estimation and inference
{[}\protect\hyperlink{ref-little2019statistical}{1}{]}. When the
missingness follows NMAR, it becomes non-ignorable. However, within this
project's scope, we will only discuss MCAR and MAR, as EM-LM will only
be justified under these two settings.

\hypertarget{missing-completely-at-random}{%
\subsection{Missing Completely at
Random}\label{missing-completely-at-random}}

If the likelihood of being observed is independent of any variables, the
missingness mechanism is MCAR. That is, \[
\Pr(R|X,  \boldsymbol\Theta) = \Pr(R| \boldsymbol\Theta).
\] MCAR is a strong ideal assumption on the mechanism, which assumes the
missingness is unrelated to all variables. It indicates that the pattern
is not affected by the studied subjects. For instance, when we research
blood samples, several samples can be contaminated during the delivery
process. Thus, we cannot collect information from these polluted
specimens. Nothing about the specimens themselves made them more or less
likely to be contaminated.

\hypertarget{missing-at-random}{%
\subsection{Missing at Random}\label{missing-at-random}}

If the likelihood of being observed depends only on those fully observed
variables, the missingness mechanism is MAR. That is, \[
\Pr(R|X, \boldsymbol\Theta) = \Pr(R|X_{obs}, \boldsymbol\Theta).
\] Compared to MCAR, MAR is a less constrained statement. Based on this
definition, MCAR can be seen as an extreme, special case of MAR
{[}\protect\hyperlink{ref-schafer2002missing}{2}{]}.

\hypertarget{methods}{%
\section{Methods}\label{methods}}

\hypertarget{complete-case-analysis}{%
\subsection{Complete-Case Analysis}\label{complete-case-analysis}}

As we have introduced in Section 1, CC analysis is a common method in
the presence of data missingness. After removing the incomplete
observations from the data set, we can perform all kinds of calculation
on the remaining data. These observations are indexed by elements in
\(S\). For all \(i \in S\), \(R_i = 1\). As a result, the likelihood
function is modified to \(L_{CC}(\boldsymbol{\Theta})\).\\
\[
\begin{aligned}
L_{CC}(\boldsymbol\Theta)
= \prod_{i \in S} \big[ f(y_i, x_i; \boldsymbol\Theta)f(z_i|y_i, x_i; \boldsymbol\Theta) \big]
\end{aligned}
\]

Then, the CC analysis obtains \(\hat{\boldsymbol\Theta}\) by maximizing
\(L_{CC}(\boldsymbol\Theta)\). There are a few circumstances where the
CC analysis can yield valid estimators. When the missingness mechanism
is MCAR, the estimator is generally unbiased and consistent because the
completeness is independent of all variables in the data set. The
complete cases can be viewed as random samples from an imagined full
data set {[}\protect\hyperlink{ref-seaman2013review}{3}{]}. Apart from
that, the CC analysis estimator can have negligible bias under a weaker
condition: the missingness mechanism is MAR, but it is independent of
the response variable given the observed predictor variables
{[}\protect\hyperlink{ref-seaman2013review}{3}{]}. CC analysis can be
straightforwardly implemented in coding by drop observations with
missing data.

\hypertarget{maximum-likelihood-em-algorithm}{%
\subsection{Maximum Likelihood EM
Algorithm}\label{maximum-likelihood-em-algorithm}}

The EM algorithm is an iterative optimization process to calculate the
maximum likelihood estimates of parameters when there exist hidden
variables or missing data
{[}\protect\hyperlink{ref-dempster1977maximum}{4}{]}. As the name
directly tells, it consists of two major steps: expectation (E-step) and
maximization (M-step). In the E-step, we derive the conditional
expectation of the log-likelihood function. The expectation is
integrated over the conditional distribution:
\[f(\text{variables}~|~\text{observed variables})\] Then, in the M-step,
we iteratively optimize the conditional expectation to obtain the
maximum likelihood estimates. The M-step can be visualized as the
following pseudo-code.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Initial guess on the parameters}
\Output{MLE}
\BlankLine
initialization $\boldsymbol{\hat{\Theta}}^{(0)}$\;
\While{k < max-iteration}{
    $\boldsymbol{\hat{\Theta}}^{(k)} = argmax~\mathbb{E}_{\boldsymbol\Theta}[\log L|\boldsymbol{\hat{\Theta}}^{(k-1)}, X_{obs}]$\;
    $e = \boldsymbol{\hat{\Theta}}^{(k)} - \boldsymbol{\hat{\Theta}}^{(k-1)}$\;
    \eIf{$\max{|e|}$ < tolerance}{
        return $\boldsymbol{\hat{\Theta}}^{(k)}$\;
    }{
        continue\;
    }
}
\caption{M-step of EM algorithm}
\end{algorithm}

\hypertarget{assumptions}{%
\subsubsection{Assumptions}\label{assumptions}}

In our linear regression setting, any ignorable missingness can happen
to \(X\) and \(y\) is fully observed. Moreover, since the EM algorithm
maximize the joint distribution of \(x_i\) and \(y_i\), we have to
explicitly assume parametric model of both variables. Therefore, the
EM-LM algorithm assume the following distribution models for both
predictor and response. \[
\begin{aligned}
x_i &\overset{iid}{\sim} \mathcal{N}(\mu, \Sigma) \\
y_i | x_i &\overset{ind}{\sim} N(\beta_0 + x^{\top}\beta, \sigma^2)
\end{aligned}
\] Therefore, the likelihood functions for the optimization fall into
one of the follows. \[
\begin{aligned}
&L_{full} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_i)\\
&L_{observed} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_{i,observed})
\end{aligned}
\]

\hypertarget{point-estimates}{%
\subsubsection{Point Estimates}\label{point-estimates}}

As the problem setting states, any element in \(X\) can be missing and
\(y\) is fully observed. To derive the conditional expectation of the
log-likelihood function, denoted as
\(J\big(\boldsymbol{\Theta}|\boldsymbol{\hat{\Theta}}^{(k-1)}\big)\), we
need to obtain the reverse conditional distribution \(f(x_i|y_i)\). \[
\begin{aligned}
f(x_i|y_i) &\propto f(y_i|x_i) f(x_i) \\
&\propto \exp\Big\{ -\frac{1}{2}x_{i}^{\top}\big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)x_i+
(\beta^{\top} \frac{y-\beta_0}{\sigma^2} + \mu^{\top}\Sigma^{-1})x_i\Big\}
\end{aligned}
\] Thus, \[
\begin{aligned}
x_i | y_i &\overset{Ind}{\sim} \mathcal{N}(\xi_i, \Phi), \\
&\text{where}
\begin{cases}
    \Phi = \big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)^{-1} \\
    \xi_i = \big( \frac{y_i-\beta_0}{\sigma^2}\beta + \Sigma^{-1}\mu \big)\Phi
\end{cases}
\end{aligned}
\] One interesting observation from the derivation is that independence
among predictor variables does not guarantee conditional independence
given the response.

Now, we can define the conditional expectation function as
\(\tilde{\mathbb{E}}(\cdot)\). \[
\tilde{\mathbb{E}}(g(x)) = 
\begin{cases}
    g(x), ~\text{if}~x~\text{observed} \\
    \mathbb{E}[g(x) |~\text{observed data}, \boldsymbol{\hat\Theta}], ~\text{else}
\end{cases}
\] Optimizing \(J\) is equivalent to optimizing
\(J* = \tilde{\mathbb{E}}\big[\sum_{i=1}^{n} \log f(y_i|x_i) + \log f(x_i)\big]\).
Meanwhile, let \(\beta^{*} = (\beta_0, \beta)\) and
\(x_i^{*} = (1, x_i)\). Then, goal became optimizing \[
\begin{aligned}
J* &= \sum_{i = 1}^{n}\tilde{\mathbb{E}}\Big\{ -\frac{1}{2} \log(|\Sigma|) - \frac{1}{2} (x_i - \mu)^{\top}\Sigma^{-1}(x_i - \mu) \\
&- \frac{1}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}(y - x^{*\top}\beta^{*})\Big\}
\end{aligned}
\] Then, we can calculate the first derivatives to optimize the
function.

\[
\begin{aligned}
\boldsymbol{\mathrm{D}}_{{\beta}^{*}}J* &= 
\sum_{i=1}^{n} - \frac{1}{\sigma^2} \{-\tilde{\mathbb{E}}(x_i^{*})y_i + \tilde{\mathbb{E}}(x_i^{*}x_i^{*\top})\beta^{*}\} \\
\boldsymbol{\mathrm{D}}_{{\sigma}^2}J* &=
\sum_{i=1}^{n} \Big\{ - \frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} \tilde{\mathbb{E}}\big((y_i-x^{*\top}\beta^{*})^2\big) \Big\}\\
\boldsymbol{\mathrm{D}}_{\mu}J* &= \Sigma^{-1} \sum_{i=1}^{n} \Big\{\tilde{\mathbb{E}}(x_i) - \mu \Big\}\\
\boldsymbol{\mathrm{D}}_{\Sigma}J* &= 
\sum_{i=1}^{n} -\frac{1}{2} \Big\{ \Sigma^{-1} - \Sigma^{-1} \tilde{\mathbb{E}}\big((x_i-\mu)^2\big) \Sigma^{-1} \Big\}
\end{aligned}
\] By setting the first derivative to 0, we obtained the closed-form
estimator. For a simple linear regression case, we can replace the
\(\mu\) and \(\Sigma\) with scalars \(\mu_1\) and \(\sigma_{1}^{2}\). It
is helpful to use \emph{The Matrix Cook}
{[}\protect\hyperlink{ref-petersen2008matrix}{5}{]} as the reference to
support linear algebra calculation during the derivation.

\hypertarget{standard-error-confidence-interval}{%
\subsubsection{Standard Error \& Confidence
Interval}\label{standard-error-confidence-interval}}

The theoretical standard error of estimates are calculated from the
observed information matrix \[ 
I_n (\hat{\boldsymbol{\Theta}}) = n \cdot \mathrm{Cov}(S(\hat{\boldsymbol{\Theta}}))
\] Then the standard error of \(\hat\Theta_i\) is calculated by the
square root of the i-th diagonal element of
\(I_n(\hat{\boldsymbol{\Theta}})^{-1}\). Due to the asymptotic normality
of the maximum likelihood estimator, we can use the standard errors to
calculate \((1-\alpha)100\%\) confidence interval (CI) for parameters.
We also implement a bootstrapping approach based on Simple Random
Sampling with Replacement as an alternative way to calculate CIs.
Considering the computation efficiency, we will not use bootstrapped CI
in the simulation studies.

\hypertarget{simulation-studies}{%
\section{Simulation Studies}\label{simulation-studies}}

We designed a series of numerical simulations to evaluate the
performance of EM-LM and compare it with the CC approach. Each instance
of the simulated dataset is obtained from a two step procedure. In the
first step, we generate the full dataset from a response model as
follows. \[
y_i = \beta_0 + x_{i}^{\top}\beta + \epsilon_i
\] where \(i = 1,...,N\).
\(\forall i, x_i \sim \mathcal{N}(\mu, \Sigma)\) and
\(\epsilon_i \sim N(0, \sigma^2)\). In the second step, we generate the
\(n \times p\) random matrix \(R\) to indicate the observation of \(X\).
Each element in the matrix, \(R_{ij}\), follows a Bernoulli distribution
with probability \(p_{ij}\). If the missingness mechanism is MCAR,
\(p_{ij}\) is a constant for all \(i\); if the missingness mechanism is
MAR, \(p_{ij}\)'s values are calculated through a logistic regression
depending on other variables, such as \(y\) or any other column of
\(X\).

\hypertarget{simple-linear-regression}{%
\subsection{Simple Linear Regression}\label{simple-linear-regression}}

In of simulations of this section, the simulated data are based on a
fixed set of population parameters shown below. \[
\mu_1 = 10,~\sigma_{1}^{2} = 25,~\beta_0 = 10,~\beta_1 = 2
\] We define \(1 - p\) as the missingness rate and \(\sigma\) as the
noise scale. These two parameters are tuned in each instance of the
simulation.

\hypertarget{varied-missingness-rate-and-noise-scale}{%
\subsubsection{Varied Missingness Rate and Noise
Scale}\label{varied-missingness-rate-and-noise-scale}}

Tables 1-4 display the estimators' performance under varied missingness
rates noise scales.

\begin{table}[h]
\caption{Missingness rate is 5\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.001 & 24.989 & 24.717 & 10.016 & 1.998\\
\hspace{1em}full & 10.001 & 24.867 & 24.722 & 10.012 & 1.999\\
\hspace{1em}observed & 10.001 & 24.858 & 24.728 & 10.01 & 1.999\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.133 & 6.564 & 6.204 & 0.669 & 0.005\\
\hspace{1em}full & 0.128 & 6.302 & 6.193 & 0.662 & 0.005\\
\hspace{1em}observed & 0.133 & 6.495 & 6.198 & 0.662 & 0.005\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.946 & 0.946 & 0.942 & 0.95 & 0.948\\
\hspace{1em}full & 0.948 & 0.945 & 0.946 & 0.954 & 0.953\\
\hspace{1em}observed & 0.942 & 0.942 & 0.947 & 0.953 & 0.953\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 20\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 9.988 & 25.029 & 24.777 & 10.006 & 2\\
\hspace{1em}full & 9.987 & 24.909 & 24.796 & 9.99 & 2.001\\
\hspace{1em}observed & 9.988 & 24.872 & 24.831 & 9.981 & 2.002\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.159 & 7.985 & 7.945 & 0.785 & 0.006\\
\hspace{1em}full & 0.134 & 6.932 & 7.868 & 0.74 & 0.006\\
\hspace{1em}observed & 0.159 & 7.885 & 7.902 & 0.743 & 0.006\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.944 & 0.943 & 0.931 & 0.951 & 0.948\\
\hspace{1em}full & 0.951 & 0.945 & 0.94 & 0.952 & 0.952\\
\hspace{1em}observed & 0.923 & 0.928 & 0.942 & 0.952 & 0.951\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 5}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.013 & 25.029 & 24.474 & 9.995 & 2\\
\hspace{1em}full & 10.008 & 24.888 & 24.547 & 9.936 & 2.005\\
\hspace{1em}observed & 10.013 & 24.778 & 24.777 & 9.885 & 2.01\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.247 & 12.943 & 11.924 & 1.368 & 0.011\\
\hspace{1em}full & 0.149 & 8.538 & 11.804 & 1.167 & 0.009\\
\hspace{1em}observed & 0.247 & 12.684 & 12.227 & 1.234 & 0.01\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.949 & 0.94 & 0.926 & 0.942 & 0.943\\
\hspace{1em}full & 0.956 & 0.948 & 0.939 & 0.946 & 0.953\\
\hspace{1em}observed & 0.877 & 0.893 & 0.943 & 0.94 & 0.947\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 10}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\sigma^2_1$ & $\sigma^2$ & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 9.987 & 25.01 & 97.84 & 10.038 & 1.994\\
\hspace{1em}full & 10.001 & 24.844 & 98.356 & 9.959 & 2.005\\
\hspace{1em}observed & 9.987 & 24.759 & 99.034 & 9.987 & 2.003\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.253 & 12.293 & 206.502 & 5.039 & 0.041\\
\hspace{1em}full & 0.192 & 10.868 & 180.224 & 3.821 & 0.031\\
\hspace{1em}observed & 0.253 & 12.047 & 181.751 & 3.897 & 0.031\\
\addlinespace[0.3em]
\multicolumn{6}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.944 & 0.941 & 0.919 & 0.952 & 0.948\\
\hspace{1em}full & 0.948 & 0.947 & 0.933 & 0.954 & 0.954\\
\hspace{1em}observed & 0.912 & 0.93 & 0.939 & 0.951 & 0.951\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\hypertarget{misspecified-predictor-distribution}{%
\subsubsection{Misspecified predictor
distribution}\label{misspecified-predictor-distribution}}

A Normal distribution is used to model a right skewed data that was
originally designed to follow a \(Exp(\lambda = 0.1)\). We assess
estimators performance under misspecification with two missingness
rates: 5\% and 50\%. Tables 5-6 display estimators' performance under
varied missingness rates when \(X\) is misspecified.

\begin{table}[h]
\caption{Missingness rate is 5\%, noise scale is 5, and misspecified $X$}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 9.998 & 2\\
\hspace{1em}full & 9.997 & 2\\
\hspace{1em}observed & 9.995 & 2.001\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.272 & 0.001\\
\hspace{1em}full & 0.272 & 0.001\\
\hspace{1em}observed & 0.273 & 0.001\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.947 & 0.949\\
\hspace{1em}full & 0.95 & 0.955\\
\hspace{1em}observed & 0.951 & 0.953\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, noise scale is 5, and misspecified $X$}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\beta_0$ & $\beta_1$\\
\midrule
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.003 & 2\\
\hspace{1em}full & 9.963 & 2.005\\
\hspace{1em}observed & 9.884 & 2.017\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.513 & 0.003\\
\hspace{1em}full & 0.497 & 0.003\\
\hspace{1em}observed & 0.576 & 0.005\\
\addlinespace[0.3em]
\multicolumn{3}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.949 & 0.942\\
\hspace{1em}full & 0.957 & 0.932\\
\hspace{1em}observed & 0.944 & 0.863\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table}

\hypertarget{multivariate-linear-regression}{%
\subsection{Multivariate Linear
Regression}\label{multivariate-linear-regression}}

In the previous simulation studies for simple linear regression, we did
experiments with MCAR missingness under various missingness rates and
noise scales. Now, in the multivariate linear regression setting, we
designed two different MAR mechanisms for \(X\) to test the performance
of the EM-LM. In of simulations of this section, the simulated data are
based on a fixed set of population parameters shown below. Moreover, we
set the missingness scale as 5 for both simulations. Thus, the only
condition we will tune is the missingness mechanism. \[
\begin{aligned}
\mu &= (10, 0, 10)^{\top} \\
\Sigma &= \text{diag}(25, 1, 25) \\
\beta_0 &= 10 \\
\beta_1 &= (1, 3, 0)^{\top}
\end{aligned}
\]

\hypertarget{mar-independent-of-y}{%
\subsubsection{MAR independent of y}\label{mar-independent-of-y}}

In this simulation, the missingness mechanism is designed as follows.
Table 7 display estimators' performance under MAR missingness that is
independent of the response variable. \[
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -40 + X_{i2} \\
p_{i2} &= 0.8
\end{aligned}
\]

\begin{table*}[hbp]
\caption{Here you can type in your caption}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\mu_2$ & $\mu_3$ & $\sigma^2_1$ & $\sigma^2_2$ & $\sigma^2_3$ & $\sigma^2$ & $\beta_0$ & $\beta_1$ & $\beta_2$ & $\beta_3$\\
\midrule
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10 & 0 & 10.002 & 24.981 & 0.907 & 25.027 & 24.309 & 9.977 & 1 & 2.998 & 0.003\\
\hspace{1em}full & 9.825 & 0 & 10.002 & 25.024 & 0.996 & 24.867 & 25.619 & 10.41 & 0.975 & 2.694 & 0.002\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.151 & 0.005 & 0.16 & 9.089 & 0.012 & 9.286 & 8.976 & 1.769 & 0.008 & 0.211 & 0.008\\
\hspace{1em}full & 0.142 & 0.005 & 0.16 & 7.181 & 0.01 & 7.819 & 6.982 & 1.222 & 0.006 & 0.13 & 0.005\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.948 & 0.949 & 0.947 & 0.923 & 0.787 & 0.926 & 0.917 & 0.946 & 0.946 & 0.949 & 0.945\\
\hspace{1em}full & 0.929 & 0.954 & 0.95 & 0.951 & 0.951 & 0.949 & 0.972 & 0.964 & 0.952 & 0.904 & 0.982\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table*}

\hypertarget{mar-dependent-of-y}{%
\subsubsection{MAR dependent of y}\label{mar-dependent-of-y}}

In this simulation, the missingness mechanism is designed as follows.
Table 8 display estimators' performance under MAR missingness that
depends on the response variable. \[
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -12 + y \\
p_{i2} &= 0.8
\end{aligned}
\]

\begin{table*}[hbp]
\caption{Here you can type in your caption}
\begingroup\fontsize{7}{9}\selectfont

\begin{tabu} to \linewidth {>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X>{\raggedright}X}
\toprule
 & $\mu_1$ & $\mu_2$ & $\mu_3$ & $\sigma^2_1$ & $\sigma^2_2$ & $\sigma^2_3$ & $\sigma^2$ & $\beta_0$ & $\beta_1$ & $\beta_2$ & $\beta_3$\\
\midrule
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Mean}}\\
\hspace{1em}cc & 10.898 & -0.002 & 9.996 & 21.331 & 0.948 & 24.895 & 19.77 & 12.985 & 0.817 & 2.446 & -0.002\\
\hspace{1em}full & 10.005 & -0.002 & 9.996 & 24.923 & 0.995 & 24.794 & 25.659 & 10.25 & 0.976 & 2.81 & 0\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Variance}}\\
\hspace{1em}cc & 0.134 & 0.005 & 0.168 & 6.973 & 0.013 & 8.853 & 5.842 & 1.51 & 0.007 & 0.167 & 0.006\\
\hspace{1em}full & 0.155 & 0.005 & 0.168 & 9.223 & 0.009 & 7.552 & 8.953 & 1.176 & 0.006 & 0.136 & 0.005\\
\addlinespace[0.3em]
\multicolumn{12}{l}{\textbf{Coverage rate}}\\
\hspace{1em}cc & 0.289 & 0.949 & 0.938 & 0.599 & 0.857 & 0.927 & 0.419 & 0.349 & 0.433 & 0.711 & 0.955\\
\hspace{1em}full & 0.946 & 0.954 & 0.942 & 0.943 & 0.951 & 0.947 & 0.964 & 0.982 & 0.953 & 0.949 & 0.982\\
\bottomrule
\end{tabu}
\endgroup{}
\end{table*}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

The simulation results corroborated our expectations that linear
regression augmented by the EM algorithm generally outperformed CC
analysis. It maximized the usage of all observed information without
dropping any incomplete observations. The simulation results provided us
with a few critical take-home messages. We realized that high
missingness rates require more iterations of M-steps to have the
algorithm achieve a convergence. Moreover, missingness, along with the
noise, weakened the estimator's efficiency. Based on the simple linear
regression setting, we found that applying EM over full likelihood
generally performs better than using it over observed likelihood.
Implementing the full likelihood, we had higher coverage probability on
the predictor parameters and slightly better efficiency in estimating
linear model coefficients. ML approach with the EM algorithm was always
more efficient than CC, and the difference became more apparent when the
degree of missingness increased.

An interesting observation was that the misspecification of the
predictor's distribution did not significantly impair the estimation on
\(\beta\) for both methods. It was reasonable that CC was not affected
as the ordinary linear regression did not require any assumption on
\(X\) and CC is theoretically unbiased when the missingness mechanism is
MCAR. On the contrary, the EM algorithm demanded explicit modeling of
the likelihood function, which involved the distribution of \(X\). In
the derivation, the estimation of \(\beta\) depended on \(\mu\) and
\(\Sigma\). Theoretically, incorrect estimation of these predictor
parameters can eventually influence the estimation of \(\beta\).
However, the EM estimator seemed to be more robust than the expectation.

In the multivariate setting, as we could manipulate more predictor
variables, we assessed the algorithm performance under two different MAR
mechanisms: MAR independent of \(y\) and MAR dependent on \(y\).
Following Seaman and White's theory
{[}\protect\hyperlink{ref-seaman2013review}{3}{]}, we also observed that
CC seemed to generate unbiased estimates when the mechanism did not
involve \(y\). However, its results became misleading when the mechanism
involved \(y\). EM-LM's results were generally more efficient. It
remained unclear that the estimates tended to be slightly biased, but
the converge probabilities were generally at the right level.

\hypertarget{reference}{%
\subsection*{Reference}\label{reference}}
\addcontentsline{toc}{subsection}{Reference}

\hypertarget{refs}{}
\begin{CSLReferences}{0}{0}
\leavevmode\vadjust pre{\hypertarget{ref-little2019statistical}{}}%
\CSLLeftMargin{{[}1{]} }%
\CSLRightInline{R.J. Little, D.B. Rubin, Statistical analysis with
missing data, John Wiley \& Sons, 2019.}

\leavevmode\vadjust pre{\hypertarget{ref-schafer2002missing}{}}%
\CSLLeftMargin{{[}2{]} }%
\CSLRightInline{J.L. Schafer, J.W. Graham, Missing data: Our view of the
state of the art., Psychological Methods. 7 (2002) 147.}

\leavevmode\vadjust pre{\hypertarget{ref-seaman2013review}{}}%
\CSLLeftMargin{{[}3{]} }%
\CSLRightInline{S.R. Seaman, I.R. White, Review of inverse probability
weighting for dealing with missing data, Statistical Methods in Medical
Research. 22 (2013) 278--295.}

\leavevmode\vadjust pre{\hypertarget{ref-dempster1977maximum}{}}%
\CSLLeftMargin{{[}4{]} }%
\CSLRightInline{A.P. Dempster, N.M. Laird, D.B. Rubin, Maximum
likelihood from incomplete data via the EM algorithm, Journal of the
Royal Statistical Society: Series B (Methodological). 39 (1977) 1--22.}

\leavevmode\vadjust pre{\hypertarget{ref-petersen2008matrix}{}}%
\CSLLeftMargin{{[}5{]} }%
\CSLRightInline{K.B. Petersen, M.S. Pedersen, others, The matrix
cookbook, Technical University of Denmark. 7 (2008) 510.}

\end{CSLReferences}

\end{document}
