---
title: "**Enhancing Linear Regression with EM algorithm to Handle Missing Data**"
subtitle: BIOSTAT 823 Final Project Report
author: |
    Scott Sun  
    GitHub repo: https://github.com/scotsun/bios823final_project
output: 
    pdf_document:
      number_sections: yes
      extra_dependencies: ["stfloats"]
geometry: margin=2cm
classoption: twocolumn
bibliography: res/report.bib
link-citations: yes
csl: res/jbi.csl
abstract: |
  Missing data is a common issue in data analysis. Unbiased and efficient estimation of the parameters governing the mean response model requires the missing data to be appropriately addressed. In this project, we derived and developed an EM algorithm for General Linear Model and compared its performance with Complete-Case Analysis. The simulation result showed that applying the EM algorithm over the full likelihood was more preferable than applying it over the observed variables' likelihood. More importantly, EM estimators were generally more efficient and less biased when the missingness pattern became more complex.
header-includes:
  - \usepackage[ruled,vlined]{algorithm2e}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
source("./res/utils.R")
# simple
high_missingness <- readRDS("../simulation_rlt/simple_linear/p50sigmaSq25.rds")
mid_missingness <- readRDS("../simulation_rlt/simple_linear/p80sigmaSq25.rds")
low_missingness <- readRDS("../simulation_rlt/simple_linear/p95sigmaSq25.rds")
high_missingness_high_noise <- readRDS("../simulation_rlt/simple_linear/p50sigmaSq100.rds")
## misspecification
high_missingness_misspec <- readRDS("../simulation_rlt/simple_linear/misspecp50sigmaSq25.rds") %>% 
  lapply(function(x) x[,-(1:2)])
low_missingness_misspec <- readRDS("../simulation_rlt/simple_linear/misspecp95sigmaSq25.rds") %>% 
  lapply(function(x) x[,-(1:2)])
# multiple
multvariate_mcar <- readRDS("../simulation_rlt/mult_linear/gamma2p80sigmaSq25.rds")
multvariate_mar <- readRDS("../simulation_rlt/mult_linear/gamma-12p80sigmaSq25.rds")
```

# Introduction
It is common for applied data analysts or researchers to retain complete observations only when the data has missingness. This approach is called complete-case (CC) analysis. It is usually the default in many software packages (i.e., `lm` in `R`). It is straightforward to understand but only sometimes valid since it may exclude potentially helpful information. Various statistical or machine learning approaches handle missingness have been developed as more justified alternatives to CC. In this project, we will derive and develop a method called **EM-LM**, which combines the Expectation-Maximization (EM) algorithm and General Linear Model. Meanwhile, we will also compare the performance of EM-LM with `R`'s default `lm` under varied conditions of degrees of missingness, noise scales, predictor being misspecified or not, and missingness mechanisms. We will demonstrate the comparisons through numerical simulations.

# Problem Setting and Notations
Assume in the General Linear Model, the sample size is $n$ and the number of predictors is $p$. In this project, we will only discuss scenarios where the predictor variable $X \in \mathbb{R}^{n \times p}$ can have missing values and the response variable $y \in \mathbb{R}^n$ is fully observed. On the observation level, $x_i$ is p-vector and $y_i$ is a scalar. We use $R \in \mathbb{R}^{n \times p}$ and $r \in \mathbb{R}^{n \times 1}$ to indicate the observation of every data value and the completeness of observation, respectively. Let $S$ represent the index set of all individuals of complete observation, and let $\bar{S}$ represent the complementary set. That is, $S = \{i: r_i = 1\}$ and $\bar{S} = \{i: r_i = 0\}$.

# Missingness Mechanisms
In the framework defined by Little and Rubin @little2019statistical, missingness mechanisms are classified into three categories: **Missing Completely at Random (MCAR)**, **Missing at Random (MAR)**, and Not Missing at Random (NMAR). Any missingness pattern following the MCAR or MAR mechanism is called **ignorable** because the pattern does not interfere with parameter estimation and inference [@little2019statistical]. When the missingness follows NMAR, it becomes non-ignorable. However, within this project's scope, we will only discuss MCAR and MAR, as EM-LM will only be justified under these two settings.

## Missing Completely at Random
If the likelihood of being observed is independent of any variables, the missingness mechanism is MCAR. That is,
$$
\Pr(R|X,  \boldsymbol\Theta) = \Pr(R| \boldsymbol\Theta).
$$
MCAR is a strong ideal assumption on the mechanism, which assumes the missingness is unrelated to all variables. It indicates that the pattern is not affected by the studied subjects. For instance, when we research blood samples, several samples can be contaminated during the delivery process. Thus, we cannot collect information from these polluted specimens. Nothing about the specimens themselves made them more or less likely to be contaminated.

## Missing at Random
If the likelihood of being observed depends only on those fully observed variables, the missingness mechanism is MAR. That is,
$$
\Pr(R|X, \boldsymbol\Theta) = \Pr(R|X_{obs}, \boldsymbol\Theta).
$$
Compared to MCAR, MAR is a less constrained statement. Based on this definition, MCAR can be seen as an extreme, special case of MAR [@schafer2002missing].

# Methods
## Complete-Case Analysis
CC analysis is a common method in the presence of data missingness. After removing the incomplete observations from the dataset, we can perform all kinds of calculation on the remaining data. These observations are indexed by elements in $S$. For all $i \in S$, $r_i = 1$. As a result, the likelihood function $L_{CC}(\boldsymbol{\Theta})$.  
$$
\begin{aligned}
L_{CC}(\boldsymbol\Theta)
= \prod_{i \in S} \big[ f(y_i, x_i; \boldsymbol\Theta)f(z_i|y_i, x_i; \boldsymbol\Theta) \big]
\end{aligned}
$$
Then, the CC analysis obtains $\hat{\boldsymbol\Theta}$ by maximizing $L_{CC}(\boldsymbol\Theta)$. There are a few circumstances where the CC analysis can yield valid estimators. When the missingness mechanism is MCAR, the estimator is generally unbiased and consistent because the completeness is independent of all variables in the data. The complete cases can be viewed as random samples from an imagined full data set [@schafer2002missing, @seaman2013review]. Apart from that, the CC analysis estimator can have negligible bias under a relative weaker condition: the missingness mechanism is MAR, but it is independent of the response variable given the observed predictor variables [@seaman2013review]. CC analysis can be straightforwardly implemented in coding by dropping observations with missing data.

## Maximum Likelihood EM Algorithm
The EM algorithm is an iterative optimization process to calculate the maximum likelihood estimates of parameters when there exist hidden variables or missing data [@dempster1977maximum]. As the name directly tells, it consists of two major steps: expectation (E-step) and maximization (M-step). In the E-step, we derive the conditional expectation of the log-likelihood function. The expectation is integrated over the conditional distribution: $$f(\text{variables}~|~\text{observed variables})$$ Then, in the M-step, we iteratively optimize the conditional expectation to obtain the maximum likelihood estimates. The M-step can be visualized as the following pseudo-code.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\Input{Initial guess on the parameters}
\Output{MLE}
\BlankLine
initialization $\boldsymbol{\hat{\Theta}}^{(0)}$\;
\While{k < max-iteration}{
    $\boldsymbol{\hat{\Theta}}^{(k)} = argmax~\mathbb{E}_{\boldsymbol\Theta}[\log L|\boldsymbol{\hat{\Theta}}^{(k-1)}, X_{obs}]$\;
    $e = \boldsymbol{\hat{\Theta}}^{(k)} - \boldsymbol{\hat{\Theta}}^{(k-1)}$\;
    \eIf{$\max{|e|}$ < tolerance}{
        return $\boldsymbol{\hat{\Theta}}^{(k)}$\;
    }{
        continue\;
    }
}
\caption{M-step of EM algorithm}
\end{algorithm} 

### Assumptions
In our linear regression setting, any ignorable missingness can happen to $X$, but $y$ is fully observed. Moreover, since the EM algorithm maximize the joint distribution of $x_i$ and $y_i$, we have to explicitly assume parametric model for both variables. Therefore, the EM-LM algorithm assume the following distributions.
$$
\begin{aligned}
x_i &\overset{iid}{\sim} \mathcal{N}(\mu, \Sigma) \\
y_i | x_i &\overset{ind}{\sim} N(\beta_0 + x^{\top}\beta, \sigma^2)
\end{aligned}
$$
The likelihood function to be optimized falls into one of the follows.
$$
\begin{aligned}
&L_{full} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_i)\\
&L_{observed} = \prod_{i = 1}^{n} f(y_i|x_i)f(x_{i,observed})
\end{aligned}
$$

### Point Estimates
As the problem setting states, any element in $X$ can be missing and $y$ is fully observed. To derive the conditional expectation of the log-likelihood function, denoted as $J\big(\boldsymbol{\Theta}|\boldsymbol{\hat{\Theta}}^{(k-1)}\big)$, we need to obtain the reverse conditional distribution $f(x_i|y_i)$.
$$
\begin{aligned}
f(x_i|y_i) &\propto f(y_i|x_i) f(x_i) \\
&\propto \exp\Big\{ -\frac{1}{2}x_{i}^{\top}\big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)x_i+
(\beta^{\top} \frac{y-\beta_0}{\sigma^2} + \mu^{\top}\Sigma^{-1})x_i\Big\}
\end{aligned}
$$
Thus,
$$
\begin{aligned}
x_i | y_i &\overset{Ind}{\sim} \mathcal{N}(\xi_i, \Phi), \\
&\text{where}
\begin{cases}
    \Phi = \big( \frac{\beta\beta^{\top}}{\sigma^2} + \Sigma^{-1} \big)^{-1} \\
    \xi_i = \big( \frac{y_i-\beta_0}{\sigma^2}\beta + \Sigma^{-1}\mu \big)\Phi
\end{cases}
\end{aligned}
$$
One interesting observation from the derivation is that independence among predictor variables does not guarantee conditional independence given the response.

Now, we can define the conditional expectation function as $\tilde{\mathbb{E}}(\cdot)$.
$$
\tilde{\mathbb{E}}(g(x)) = 
\begin{cases}
    g(x), ~\text{if}~x~\text{observed} \\
    \mathbb{E}[g(x) |~\text{observed data}, \boldsymbol{\hat\Theta}], ~\text{else}
\end{cases}
$$
Optimizing $J$ is equivalent to optimizing $J* = \tilde{\mathbb{E}}\big[\sum_{i=1}^{n} \log f(y_i|x_i) + \log f(x_i)\big]$. Meanwhile, let $\beta^{*} = (\beta_0, \beta)$ and $x_i^{*} = (1, x_i)$. Then, the goal became optimizing
$$
\begin{aligned}
J* &= \sum_{i = 1}^{n}\tilde{\mathbb{E}}\Big\{ -\frac{1}{2} \log(|\Sigma|) - \frac{1}{2} (x_i - \mu)^{\top}\Sigma^{-1}(x_i - \mu) \\
&- \frac{1}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}(y - x^{*\top}\beta^{*})\Big\}
\end{aligned}
$$
We can calculate the first derivatives to optimize the function.
$$
\begin{aligned}
\boldsymbol{\mathrm{D}}_{{\beta}^{*}}J* &= 
\sum_{i=1}^{n} - \frac{1}{\sigma^2} \{-\tilde{\mathbb{E}}(x_i^{*})y_i + \tilde{\mathbb{E}}(x_i^{*}x_i^{*\top})\beta^{*}\} \\
\boldsymbol{\mathrm{D}}_{{\sigma}^2}J* &=
\sum_{i=1}^{n} \Big\{ - \frac{1}{2\sigma^2} + \frac{1}{2\sigma^4} \tilde{\mathbb{E}}\big((y_i-x^{*\top}\beta^{*})^2\big) \Big\}\\
\boldsymbol{\mathrm{D}}_{\mu}J* &= \Sigma^{-1} \sum_{i=1}^{n} \Big\{\tilde{\mathbb{E}}(x_i) - \mu \Big\}\\
\boldsymbol{\mathrm{D}}_{\Sigma}J* &= 
\sum_{i=1}^{n} -\frac{1}{2} \Big\{ \Sigma^{-1} - \Sigma^{-1} \tilde{\mathbb{E}}\big((x_i-\mu)^2\big) \Sigma^{-1} \Big\}
\end{aligned}
$$
By setting the first derivative to 0, we obtained the closed-form estimator. For a simple linear regression case, we can replace the $\mu$ and $\Sigma$ with scalars $\mu_1$ and $\sigma_{1}^{2}$. It is helpful to use *The Matrix Cook* [@petersen2008matrix] as the reference to support linear algebra calculation during the derivation.

### Standard Error & Confidence Interval
The theoretical standard error of estimates are calculated from the observed information matrix
$$ 
I_n (\hat{\boldsymbol{\Theta}}) = n \cdot \mathrm{Cov}(S(\hat{\boldsymbol{\Theta}}))
$$
Then the standard error of $\hat\Theta_i$ is calculated by the square root of the i-th diagonal element of $I_n(\hat{\boldsymbol{\Theta}})^{-1}$. Due to the asymptotic normality of the maximum likelihood estimator, we can use the standard errors to calculate $(1-\alpha)100\%$ confidence interval (CI) for parameters. We also implement a bootstrapping approach based on Simple Random Sampling with Replacement as an alternative way to calculate CIs. Considering the computation efficiency, we will not use bootstrapped CI in the simulation studies.

# Simulation Studies

We designed a series of numerical simulations to evaluate the performance of EM-LM and compare it with the CC approach. Each instance of the simulated dataset is obtained from a two step procedure. In the first step, we generate the full dataset from a response model as follows.
$$
y_i = \beta_0 + x_{i}^{\top}\beta + \epsilon_i
$$
where $i = 1,...,N$. $\forall i, x_i \sim \mathcal{N}(\mu, \Sigma)$ and $\epsilon_i \sim N(0, \sigma^2)$. In the second step, we generate the $n \times p$ random matrix $R$ to indicate the observation of $X$. Each element in the matrix, $R_{ij}$, follows a Bernoulli distribution with probability $p_{ij}$. If the missingness mechanism is MCAR, $p_{ij}$ is a constant for all $i$; if the missingness mechanism is MAR, $p_{ij}$'s values are calculated through a logistic regression depending on other variables, such as $y$ or any other column of $X$.

## Simple Linear Regression
In the simulations of this section, the simulated data are based on a fixed set of population parameters shown below.
$$
\mu_1 = 10,~\sigma_{1}^{2} = 25,~\beta_0 = 10,~\beta_1 = 2
$$
We define $1 - p$ as the missingness rate and $\sigma$ as the noise scale. These two parameters are tuned in each instance of the simulation.

### Varied Missingness Rate and Noise Scale
Tables 1-4 display the estimators' performance under varied missingness rates noise scales.

\begin{table}[h]
\caption{Missingness rate is 5\%, and noise scale is 5}
```{r, echo=FALSE}
summarizing_table(low_missingness)
```
\end{table}

\begin{table}[h]
\caption{Missingness rate is 20\%, and noise scale is 5}
```{r, echo=FALSE}
summarizing_table(mid_missingness)
```
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 5}
```{r, echo=FALSE}
summarizing_table(high_missingness)
```
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, and noise scale is 10}
```{r, echo=FALSE}
summarizing_table(high_missingness_high_noise)
```
\end{table}

### Misspecified predictor distribution
A Normal distribution is used to model a right skewed data that was originally designed to follow a $Exp(\lambda = 0.1)$. We assess estimators performance under misspecification with two missingness rates: 5% and 50%. Tables 5-6 display estimators' performance under varied missingness rates when $X$ is misspecified.

\begin{table}[h]
\caption{Missingness rate is 5\%, noise scale is 5, and misspecified $X$}
```{r, echo=FALSE}
summarizing_table(low_missingness_misspec, only_beta = TRUE)
```
\end{table}

\begin{table}[h]
\caption{Missingness rate is 50\%, noise scale is 5, and misspecified $X$}
```{r, echo=FALSE}
summarizing_table(high_missingness_misspec, only_beta = TRUE)
```
\end{table}

## Multivariate Linear Regression
In the previous simulation studies for simple linear regression, we did experiments with MCAR missingness under various missingness rates and noise scales as well as whether or not misspecifying predictor. Now, we designed two different MAR mechanisms for $X$ to test the performance of the EM-LM in the multivariate linear regression setting. In simulations of this section, the simulated data are based on a fixed set of population parameters shown below. Moreover, we set the missingness scale as 5 for both simulations. Thus, the only condition we will tune is the missingness mechanism.
$$
\begin{aligned}
\mu &= (10, 0, 10)^{\top} \\
\Sigma &= \text{diag}(25, 1, 25) \\
\beta_0 &= 10 \\
\beta &= (1, 3, 0)^{\top}
\end{aligned}
$$

### MAR independent of y
In this simulation, the missingness mechanism is designed as follows. Table 7 display estimators' performance under MAR missingness that is independent of the response variable.
$$
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -40 + X_{i2} \\
p_{i2} &= 0.8
\end{aligned}
$$

\begin{table*}[hbp]
\caption{Missingness Mechanism is MCAR}
```{r, echo=FALSE}
summarizing_table(multvariate_mcar, p = 3, num_methods = 2, latex_opt = NULL)
```
\end{table*}

### MAR dependent of y
In this simulation, the missingness mechanism is designed as follows. Table 8 display estimators' performance under MAR missingness that depends on the response variable.
$$
\begin{aligned}
\mathrm{logit}(p_{i1}) &= -12 + y \\
p_{i2} &= 0.8
\end{aligned}
$$

\begin{table*}[hbp]
\caption{Missingness Mechanism is MAR}
```{r, echo=FALSE}
summarizing_table(multvariate_mar, p = 3, num_methods = 2, latex_opt = NULL)
```
\end{table*}

# Discussion & Conclusion
The simulation results corroborated our expectations that linear regression augmented by the EM algorithm generally outperformed CC analysis. It maximized the usage of all observed information without dropping any incomplete observations. The simulation results provided us with a few critical take-home messages. We realized that high missingness rates require more iterations of M-steps to have the algorithm achieve a convergence. Moreover, missingness, along with the noise, weakened the estimator's efficiency. Based on the simple linear regression setting, we found that applying EM over full likelihood generally performs better than using it over the observed likelihood. Implementing the full likelihood, we had higher coverage probability on the predictor parameters and slightly better efficiency in estimating linear model coefficients. ML approach with the EM algorithm was always more efficient than CC, and the difference became more apparent when the degree of missingness increased. 

An interesting observation was that the misspecification of the predictor's distribution did not significantly impair the estimation on $\beta$ for both methods. It was reasonable that CC was not affected as the ordinary linear regression did not require any assumption on $X$, and CC was theoretically unbiased when the missingness mechanism is MCAR. On the contrary, the EM algorithm demanded explicit modeling of the likelihood function. It involved the distribution of $X$. In the derivation, the estimation of $\beta$ depended on $\mu$ and $\Sigma$. Theoretically, incorrect estimation of these predictor parameters can eventually influence the estimation of $\beta$. However, the EM estimator seemed to be more robust than it was expected. The bias was negligible in practice.

In the multivariate setting, as we could manipulate more predictor variables, we assessed the algorithm performance under two different MAR mechanisms: MAR independent of $y$ and MAR dependent of $y$. Following Seaman and White's theory [@seaman2013review], we also observed that CC estimator's bias is negligible when the mechanism did not involve $y$. However, its results became misleading when the mechanism involved $y$. EM-LM's results were generally more efficient. However, it remained unclear that the estimates tended to be slightly biased, but the converge probabilities were generally at the right level.

## Reference {-}
